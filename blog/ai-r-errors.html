<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>What 713 R Errors Taught Me About AI Coding Assistants | Siyao Zheng</title>
    <meta name="description" content="I mined 1,237 AI coding sessions for every R error. The finding: AI makes the exact same mistakes human researchers do â€” variable name mismatches, path issues, type surprises. Here's the data.">
    <script defer data-domain="siyaozheng.org" src="https://plausible.io/js/script.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #fafafa;
            --bg-alt: #f5f5f5;
            --accent: #7c3aed;
            --accent-light: #8b5cf6;
            --text: #171717;
            --text-secondary: #737373;
            --border: #e5e5e5;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }
        html { scroll-behavior: smooth; }

        body {
            font-family: 'Inter', sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            font-weight: 400;
        }

        /* Navigation */
        nav {
            position: fixed;
            top: 0; left: 0; right: 0;
            padding: 1.5rem 4rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            background: rgba(250, 250, 250, 0.95);
            backdrop-filter: blur(10px);
            z-index: 1000;
            border-bottom: 1px solid var(--border);
        }

        .nav-name {
            font-size: 1.4rem;
            font-weight: 600;
            color: var(--text);
            letter-spacing: 0.02em;
            text-decoration: none;
        }

        .nav-links { display: flex; gap: 2.5rem; }

        .nav-links a {
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 0.12em;
            color: var(--text-secondary);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
            position: relative;
        }

        .nav-links a::after {
            content: '';
            position: absolute;
            bottom: -4px; left: 0;
            width: 0; height: 1px;
            background: var(--accent);
            transition: width 0.3s ease;
        }

        .nav-links a:hover { color: var(--text); }
        .nav-links a:hover::after { width: 100%; }

        /* Article Layout */
        .article-header {
            max-width: 720px;
            margin: 0 auto;
            padding: 8rem 2rem 3rem;
        }

        .article-meta {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.2em;
            color: var(--accent);
            margin-bottom: 1.5rem;
            font-weight: 500;
        }

        .article-header h1 {
            font-size: 2.8rem;
            font-weight: 700;
            line-height: 1.15;
            color: var(--text);
            margin-bottom: 1.5rem;
            letter-spacing: -0.02em;
        }

        .article-subtitle {
            font-size: 1.15rem;
            color: var(--text-secondary);
            line-height: 1.7;
            max-width: 620px;
        }

        /* Article Body */
        .article-body {
            max-width: 720px;
            margin: 0 auto;
            padding: 0 2rem 6rem;
        }

        .article-body p {
            font-size: 1.05rem;
            line-height: 1.85;
            color: var(--text);
            margin-bottom: 1.5rem;
        }

        .article-body h2 {
            font-size: 1.8rem;
            font-weight: 700;
            color: var(--text);
            margin: 3rem 0 1.2rem;
            letter-spacing: -0.01em;
        }

        .article-body h3 {
            font-size: 1.3rem;
            font-weight: 600;
            color: var(--text);
            margin: 2.5rem 0 1rem;
        }

        .article-body a {
            color: var(--accent);
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-color 0.3s ease;
        }

        .article-body a:hover {
            border-bottom-color: var(--accent);
        }

        .article-body ul, .article-body ol {
            margin: 0 0 1.5rem 1.5rem;
            font-size: 1.05rem;
            line-height: 1.85;
        }

        .article-body li { margin-bottom: 0.5rem; }
        .article-body strong { font-weight: 600; }

        /* Code */
        .article-body code {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.88rem;
            background: var(--bg-alt);
            padding: 0.15rem 0.4rem;
            border-radius: 3px;
            border: 1px solid var(--border);
        }

        .article-body pre {
            background: #1e1e2e;
            color: #cdd6f4;
            padding: 1.5rem;
            border-radius: 6px;
            overflow-x: auto;
            margin: 1.5rem 0 2rem;
            line-height: 1.6;
        }

        .article-body pre code {
            background: none;
            border: none;
            padding: 0;
            font-size: 0.85rem;
            color: inherit;
        }

        /* Callout */
        .callout {
            border-left: 3px solid var(--accent);
            padding: 1.2rem 1.5rem;
            margin: 1.5rem 0 2rem;
            background: var(--bg-alt);
        }

        .callout p {
            margin-bottom: 0;
            font-size: 0.95rem;
            color: var(--text-secondary);
        }

        .callout p:first-child {
            font-weight: 600;
            color: var(--text);
            margin-bottom: 0.5rem;
        }

        /* Table */
        .data-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0 2rem;
            font-size: 0.95rem;
        }

        .data-table th {
            text-align: left;
            padding: 0.8rem 1rem;
            border-bottom: 2px solid var(--text);
            font-weight: 600;
            font-size: 0.8rem;
            text-transform: uppercase;
            letter-spacing: 0.08em;
        }

        .data-table td {
            padding: 0.8rem 1rem;
            border-bottom: 1px solid var(--border);
            vertical-align: top;
        }

        .data-table .num {
            font-family: 'JetBrains Mono', monospace;
            font-weight: 500;
            text-align: right;
        }

        .data-table .accent {
            color: var(--accent);
        }

        /* Error block */
        .error-block {
            background: #fef2f2;
            border: 1px solid #fecaca;
            border-radius: 6px;
            padding: 1.2rem 1.5rem;
            margin: 1.5rem 0 2rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.88rem;
            color: #991b1b;
            line-height: 1.6;
        }

        /* Big stat */
        .big-stat {
            display: flex;
            gap: 2rem;
            margin: 2rem 0;
            padding: 1.5rem 0;
            border-top: 1px solid var(--border);
            border-bottom: 1px solid var(--border);
        }

        .big-stat-item {
            flex: 1;
            text-align: center;
        }

        .big-stat-number {
            font-family: 'JetBrains Mono', monospace;
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--accent);
            line-height: 1.2;
        }

        .big-stat-label {
            font-size: 0.8rem;
            color: var(--text-secondary);
            text-transform: uppercase;
            letter-spacing: 0.08em;
            margin-top: 0.3rem;
        }

        /* Timeline */
        .timeline {
            position: relative;
            padding-left: 2rem;
            margin: 1.5rem 0 2rem;
        }

        .timeline::before {
            content: '';
            position: absolute;
            left: 0; top: 0; bottom: 0;
            width: 2px;
            background: var(--border);
        }

        .timeline-item {
            position: relative;
            padding-bottom: 1.5rem;
        }

        .timeline-item::before {
            content: '';
            position: absolute;
            left: -2rem;
            top: 0.45rem;
            width: 8px; height: 8px;
            background: var(--accent);
            border-radius: 50%;
        }

        .timeline-date {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            color: var(--accent);
            margin-bottom: 0.2rem;
        }

        .timeline-item p {
            font-size: 0.95rem;
            margin-bottom: 0;
            line-height: 1.6;
        }

        .timeline-count {
            font-family: 'JetBrains Mono', monospace;
            font-weight: 600;
        }

        /* Separator */
        .article-body hr {
            border: none;
            border-top: 1px solid var(--border);
            margin: 3rem 0;
        }

        /* Footer */
        footer {
            padding: 2rem 4rem;
            text-align: center;
            border-top: 1px solid var(--border);
            font-size: 0.85rem;
            color: var(--text-secondary);
        }

        footer a { color: var(--accent); text-decoration: none; }

        /* Responsive */
        @media (max-width: 1024px) {
            nav { padding: 1rem 2rem; }
            .nav-links { gap: 1.5rem; }
            .nav-links a { font-size: 0.75rem; }
        }

        @media (max-width: 600px) {
            nav { padding: 1rem 1.5rem; }
            .nav-name { font-size: 1.2rem; }
            .nav-links { display: none; }
            .article-header { padding: 7rem 1.5rem 2rem; }
            .article-header h1 { font-size: 2rem; }
            .article-body { padding: 0 1.5rem 4rem; }
            .big-stat { flex-direction: column; gap: 1rem; }
            .big-stat-number { font-size: 2rem; }
            footer { padding: 1.5rem; font-size: 0.8rem; }
        }
    </style>
    <link rel="stylesheet" href="comments/comments.css">
</head>
<body>
    <nav>
        <a class="nav-name" href="/">Siyao Zheng</a>
        <div class="nav-links">
            <a href="/#publications">Publications</a>
            <a href="/#teaching">Teaching</a>
            <a href="/#cv">CV</a>
            <a href="/blog/">Blog</a>
            <a href="/#contact">Contact</a>
        </div>
    </nav>

    <header class="article-header">
        <div class="article-meta">February 12, 2026 &middot; AI for Social Science</div>
        <h1>What 713 R Errors Taught Me About AI Coding Assistants</h1>
        <p class="article-subtitle">
            I mined 1,237 AI coding sessions for every R error. The finding: AI doesn't make exotic mistakes. It makes <em>your</em> mistakes. Variable typos, broken paths, type surprises. And R is the reason why.
        </p>
    </header>

    <article class="article-body">

        <p>
            Over the past month, I used <a href="https://docs.anthropic.com/en/docs/claude-code/overview">Claude Code</a> (Anthropic's AI coding assistant) to help build a large R-based research pipeline. Survey data from multiple sources, latent variable models, multiple imputation, regression analysis. The kind of project that grows to 40+ R files and hundreds of variables scattered across config files and model specifications.
        </p>

        <p>
            Claude Code logs every session as a structured JSON transcript. 1,237 sessions over 33 days, across all my research projects. I wrote a Python script to extract every R error from those transcripts, parsing stderr output and matching error patterns. 713 unique errors across 168 sessions.
        </p>

        <p>
            I expected to find exotic failures &mdash; hallucinated function names, impossible syntax, the kind of mistakes only an AI would make. Instead, I found my own mistakes staring back at me.
        </p>

        <h2>The Numbers</h2>

        <div class="big-stat">
            <div class="big-stat-item">
                <div class="big-stat-number">1,237</div>
                <div class="big-stat-label">Coding Sessions</div>
            </div>
            <div class="big-stat-item">
                <div class="big-stat-number">713</div>
                <div class="big-stat-label">R Errors</div>
            </div>
            <div class="big-stat-item">
                <div class="big-stat-number">86%</div>
                <div class="big-stat-label">Error-Free Sessions</div>
            </div>
        </div>

        <p>
            86% of sessions produced zero R errors. The AI is not constantly breaking things. But those 713 errors across the remaining 14% tell a story, and the story is not about AI.
        </p>

        <table class="data-table">
            <thead>
                <tr>
                    <th>Error Category</th>
                    <th class="num">Count</th>
                    <th class="num">%</th>
                    <th>In Plain English</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Variable/object not found</td>
                    <td class="num accent">160</td>
                    <td class="num">22%</td>
                    <td>Renamed in one file, forgot another</td>
                </tr>
                <tr>
                    <td>Type and dimension errors</td>
                    <td class="num accent">137</td>
                    <td class="num">19%</td>
                    <td>haven_labelled, subscript out of bounds, factor/numeric confusion</td>
                </tr>
                <tr>
                    <td>File path and IO errors</td>
                    <td class="num accent">123</td>
                    <td class="num">17%</td>
                    <td>Spaces in paths, wrong root directory, corrupt files</td>
                </tr>
                <tr>
                    <td>Package and API issues</td>
                    <td class="num">65</td>
                    <td class="num">9%</td>
                    <td>Missing package, API renames, Quarto/igraph</td>
                </tr>
                <tr>
                    <td>Syntax errors</td>
                    <td class="num">47</td>
                    <td class="num">7%</td>
                    <td>Escape characters, unexpected symbols</td>
                </tr>
                <tr>
                    <td>Runtime logic errors</td>
                    <td class="num">44</td>
                    <td class="num">6%</td>
                    <td>if() on NA, pipe chain failures, grid/ggplot</td>
                </tr>
                <tr>
                    <td>Wrong function arguments</td>
                    <td class="num">29</td>
                    <td class="num">4%</td>
                    <td>Unused arguments, misspecified options</td>
                </tr>
                <tr>
                    <td>Infrastructure failures</td>
                    <td class="num">51</td>
                    <td class="num">7%</td>
                    <td>HPC workers, targets lock, C++ compilation</td>
                </tr>
                <tr>
                    <td>Numerical/convergence</td>
                    <td class="num">24</td>
                    <td class="num">3%</td>
                    <td>Singular matrix, memory limits, hIRT initialization</td>
                </tr>
                <tr>
                    <td>Known pitfalls (mice)</td>
                    <td class="num">7</td>
                    <td class="num">1%</td>
                    <td>Documented bugs, hit repeatedly</td>
                </tr>
                <tr>
                    <td>Missing function/method</td>
                    <td class="num">25</td>
                    <td class="num">4%</td>
                    <td>Function removed or not exported</td>
                </tr>
            </tbody>
        </table>

        <p>
            The top three categories &mdash; wrong names, wrong types, wrong paths &mdash; account for 58% of all errors. Every R programmer will recognize them immediately. They share a single root cause: <strong>R gives you no feedback about code correctness until the code actually runs.</strong> This is what the rest of this post is about.
        </p>

        <h2>R Has No Compiler (And That Changes Everything)</h2>

        <p>
            The single largest category: 160 naming errors, 22% of the total. The AI renames a variable in one file &mdash; say, changing <code>income_level</code> to <code>income_percentile</code> in the data cleaning script &mdash; and misses a reference downstream. A regression formula, a config file, a visualization three pipeline steps later.
        </p>

        <div class="error-block">
            Error: Can't subset columns that don't exist.<br>
            x Column `income_level` doesn't exist.
        </div>

        <div class="error-block">
            Error in eval(expr, envir, enclos):<br>
            &nbsp;&nbsp;object 'birthyear' not found
        </div>

        <p>
            This is not an AI-specific problem. It is the most common mistake every R programmer makes. I've done it myself more times than I'd like to admit. But it reveals something fundamental about why R is different from the languages where AI coding assistants shine.
        </p>

        <p>
            In Python, referencing an undefined name raises <code>NameError</code> immediately. In Go or Rust, the compiler refuses to build. In R, your pipeline hums along for thirty minutes &mdash; loading data, fitting models, running imputation &mdash; and only crashes at minute 31 when it finally reaches the line with the wrong name.
        </p>

        <pre><code># This runs for 30 minutes before failing
data &lt;- load_all_surveys()          # 2 min
data &lt;- harmonize_variables(data)   # 3 min
models &lt;- fit_irt_models(data)      # 10 min
imps &lt;- run_mice(models, m = 20)   # 15 min
results &lt;- run_regressions(imps)
# Error: object 'income_level' not found   # minute 31</code></pre>

        <p>
            The AI has no way to "compile" R code. It writes the edit, it looks correct, and nothing validates it until runtime. This is why naming errors were the <em>only</em> category that persisted across the entire month. Type errors were fixed once and stayed fixed. Path issues were resolved and didn't return. Only naming errors kept recurring, from the first week to the last day, because R's lack of static analysis is a permanent condition.
        </p>

        <div class="callout">
            <p>Chronic vs. acute</p>
            <p>Most error categories follow an "acute" pattern: they cluster around a specific refactoring, get fixed, and don't return. The <code>haven_labelled</code> type errors appeared in late January, were resolved, and never came back. The <code>mice</code> configuration bugs, same story. Only naming errors are <em>chronic</em> &mdash; a permanent condition of working in a language with no compile step.</p>
        </div>

        <h2>Your Data Is Lying About Its Type</h2>

        <p>
            If you're a quantitative social scientist, this section is about you specifically.
        </p>

        <p>
            137 errors &mdash; 19% of the total &mdash; came from type and dimension mismatches. The most revealing variety involves survey data. When you load a Stata <code>.dta</code> file with <code>haven::read_dta()</code>, what you get back is not a normal data frame. The columns carry invisible metadata &mdash; value labels, variable labels, format specifications &mdash; encoded as a special <code>haven_labelled</code> type. It <em>looks</em> like a numeric vector. It prints like one. It isn't one.
        </p>

        <div class="error-block">
            Error: &lt;haven_labelled&gt; - &lt;haven_labelled&gt; is not permitted
        </div>

        <div class="error-block">
            Error: Can't combine &lt;haven_labelled&gt; and &lt;double&gt;.
        </div>

        <p>
            Basic arithmetic fails. <code>bind_rows()</code> fails. The data looks numeric but doesn't behave numerically. The fix is a single function call &mdash; <code>haven::zap_labels()</code> &mdash; but you have to know to call it <em>immediately</em> after loading, before the labelled values propagate through the pipeline. Three files and two hours later, something breaks with an incomprehensible type error, and the root cause is a data type that never should have existed past line 10 of your cleaning script.
        </p>

        <p>
            This is a trap that only affects people who work with survey data stored in Stata or SPSS format &mdash; which is to say, a large fraction of quantitative social scientists. The AI doesn't know about <code>haven_labelled</code> unless it's been told. It sees a data frame, assumes it's a normal data frame, and writes perfectly reasonable code that fails for invisible reasons.
        </p>

        <p>
            The broader category includes other R type traps: subscript-out-of-bounds after unexpected filtering, factor/numeric confusion when columns were silently coerced, dimension mismatches after merges that dropped rows. All invisible until runtime. All correct-looking code meeting incorrect-at-runtime data.
        </p>

        <h2>The Gap Between Your Code and Your Computer</h2>

        <p>
            174 errors &mdash; 24% of the total &mdash; have nothing to do with analytical logic. They come from the messy reality of how research computing actually works.
        </p>

        <p>
            The biggest sub-category: file paths (123 errors). If you're on macOS and your project lives in Dropbox, you probably have a path like:
        </p>

        <pre><code>/Users/you/Dropbox/My Research Project/code/R/model.R</code></pre>

        <p>
            That space in "My Research Project" is a ticking time bomb. Inside R, <code>here::here()</code> handles it fine. But the moment a path gets passed to a shell command via <code>system()</code> or <code>Rscript</code>, the space breaks things:
        </p>

        <div class="error-block">
            Fatal error: cannot open file '/Users/you/Dropbox/My': No such file or directory
        </div>

        <p>
            The AI writes the <code>system()</code> call without quoting the path &mdash; the same mistake I've made dozens of times. The fix is always the same: wrap the path in quotes. But the AI doesn't retain this lesson across sessions, because each new <code>system()</code> call is a new context. Separately, R's <code>here::here()</code> function can resolve to the <em>wrong</em> project root when a <code>renv.lock</code> file sits in a subdirectory. In our project, this sent every file lookup to <code>code/Data/</code> instead of <code>Data/</code>. A symlink fixed it, but the AI had to rediscover the problem. Multiple times.
        </p>

        <p>
            Then there's infrastructure (51 errors): the code is correct, but the environment fails. HPC workers crash mid-computation. Network connections drop between nodes. The <a href="https://docs.ropensci.org/targets/">targets</a> pipeline manager accumulates stale lock files that block new runs. C++ dependencies fail to compile on a remote cluster's older toolchain. None of this is the AI's fault. But <strong>the AI cannot distinguish "my code is wrong" from "the infrastructure had a hiccup"</strong> &mdash; both produce identical-looking errors in the log, and both send it down the wrong debugging path.
        </p>

        <h2>What the AI Remembers (and What It Doesn't)</h2>

        <p>
            Seven errors, barely 1% of the total, but they reveal something important about how AI coding assistants actually work.
        </p>

        <p>
            The <code>mice</code> package for multiple imputation has a known quirk: when using its parallel variant <code>futuremice()</code>, you must <em>not</em> pass the <code>printFlag</code> argument, because the function passes it internally. Pass it anyway, and R complains:
        </p>

        <div class="error-block">
            Error: formal argument "printFlag" matched by multiple actual arguments
        </div>

        <p>
            This was documented in our project notes from the start. Yet the AI kept re-introducing <code>printFlag = FALSE</code> every time it touched the imputation code for other reasons. Each time, from the AI's perspective, passing <code>printFlag = FALSE</code> to a <code>mice</code>-like function is the obviously correct thing to do &mdash; it has seen thousands of examples of exactly this pattern in its training data.
        </p>

        <p>
            This is the tension at the heart of AI-assisted coding: <strong>the model has strong priors from training data that can override project-specific knowledge.</strong> Our project note saying "don't pass printFlag to futuremice" is one instruction competing against an overwhelming statistical prior. The prior wins, repeatedly, until the code is structured to make the error <em>impossible</em> rather than merely inadvisable. For researchers relying on AI assistants, the lesson is architectural: don't tell the AI what not to do; design your code so the wrong thing can't be done.
        </p>

        <h2>Errors Follow the Calendar</h2>

        <p>
            Errors are not uniformly distributed. They cluster around major refactoring events:
        </p>

        <div class="timeline">
            <div class="timeline-item">
                <div class="timeline-date">Feb 1 &mdash; <span class="timeline-count">94 errors</span></div>
                <p>Full-scale imputation run + regression model rewrite</p>
            </div>
            <div class="timeline-item">
                <div class="timeline-date">Jan 31 &mdash; <span class="timeline-count">88 errors</span></div>
                <p>Unified loading of 33 surveys into a single function + new analysis module</p>
            </div>
            <div class="timeline-item">
                <div class="timeline-date">Feb 10 &mdash; <span class="timeline-count">66 errors</span></div>
                <p>Visualization refactoring + beamer presentation pipeline</p>
            </div>
            <div class="timeline-item">
                <div class="timeline-date">Feb 2 &mdash; <span class="timeline-count">55 errors</span></div>
                <p>First full-scale multiple imputation run (m=20, distributed across HPC)</p>
            </div>
        </div>

        <p>
            Four days, 42% of all errors. The pattern is clear: errors spike when the project is being restructured, not when the AI is doing routine analytical work. And the errors that dominate during refactoring are almost entirely naming errors &mdash; the AI restructures code correctly in the files it touches, but misses references in files it didn't think to update. This is the same blind spot every human programmer has. You remember to update the three files you have open. You forget about the config file you last touched two weeks ago.
        </p>

        <h2>So What?</h2>

        <p>
            Here's what I keep coming back to: <strong>AI coding errors in R are not AI-specific errors. They are R-specific errors.</strong> The AI doesn't hallucinate function names or invent impossible syntax. It makes the same mistakes I make, for the same structural reasons.
        </p>

        <p>
            R is dynamically typed, lazily evaluated, and has no compile step. A misspelled variable is only caught when the line executes, potentially hours into a pipeline. A type mismatch &mdash; <code>haven_labelled</code> vs. <code>double</code> &mdash; is invisible until an operation fails. A file path with a space in it works inside R but breaks when passed to the shell. None of these have compile-time equivalents. There is no way to "build" a multi-file R project and check for consistency before running it.
        </p>

        <p>
            These aren't bugs in R. They are design choices that prioritize interactivity and flexibility &mdash; exactly what makes R excellent for exploratory data analysis. But they also mean that neither humans nor AI gets any structural feedback about code correctness until something breaks at runtime. In languages with static tooling (TypeScript, Rust, Go), the same AI assistant makes far fewer of these errors, because the compiler catches them before execution. R doesn't have that feedback loop.
        </p>

        <div class="callout">
            <p>What I'm trying next</p>
            <p>If the AI's biggest weakness in R is the same as mine &mdash; no structural feedback before runtime &mdash; then the solution probably isn't making the AI smarter. It's giving it better tools. R's <a href="https://github.com/REditorSupport/languageserver">Language Server</a> already provides some of this for IDE users: undefined variable detection, cross-file references, scope-aware renaming. I've been experimenting with making these same capabilities available to the AI assistant directly. Whether that actually reduces the error rate is a question I'll take up in a future post.</p>
        </div>

        <hr>

        <p>
            <em>Siyao Zheng is an Assistant Professor at the School of International and Public Affairs, Shanghai Jiao Tong University. His research focuses on AI for Social Science and Digital Politics. The error data and extraction script are available on <a href="https://github.com/SiyaoZheng/">GitHub</a>.</em>
        </p>

    </article>

    <section id="comments" data-slug="ai-r-errors"></section>

    <footer>
        <p>&copy; 2026 Siyao Zheng &middot; <a href="/">Home</a></p>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/@supabase/supabase-js@2/dist/umd/supabase.min.js"></script>
    <script src="comments/profanity-filter.js"></script>
    <script src="comments/comments.js"></script>
</body>
</html>
